# SmartEd Hugging Face Integration

This guide explains how to set up the SmartEd backend to use Hugging Face Inference API instead of Google's Gemini.

## Setup Steps

1. **Create a Hugging Face Account**:
   - Go to [https://huggingface.co/join](https://huggingface.co/join) to create a free account

2. **Generate an API Token**:
   - Once logged in, go to [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)
   - Click "New Token", give it a name like "SmartEd", and select "Read" role
   - Copy the generated token

3. **Set Up Environment Variable**:
   - Create or edit your `.env` file in the backend directory
   - Add `HF_API_KEY=your-token-here` (replace with your actual token)

## Models Used

The implementation uses completely free models with no usage costs:

- **Summarization**: `google/flan-t5-small` - A small T5 model fine-tuned for summarization tasks
- **Quiz Generation**: `gpt2` - The base GPT-2 model that can generate text for quiz questions

## Limitations

Since we're using free models, there are some limitations:

1. **Quality**: The summaries and quizzes won't be as high quality as those generated by paid models
2. **Structure**: The quiz output may need post-processing as smaller models can't reliably generate structured JSON
3. **Length**: Input and output length are limited with these smaller models

## Upgrading

If you decide to improve quality in the future, you can easily update the model constants in `ai.py`:

```python
SUMMARY_MODEL = "facebook/bart-large-cnn"  # Better summarization
QUIZ_MODEL = "meta-llama/Llama-3.2-1B-Instruct"  # Better quiz generation
```

Note that using better models might require a paid subscription or higher API usage limits.

## Testing

You can test the functionality by running:

```
pip install -r requirements.txt
python ai.py
```

This will run the test functions at the bottom of the ai.py file with a sample transcript.